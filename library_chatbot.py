# -*- coding: utf-8 -*-
import os
import sys
import hashlib
import streamlit as st
from pathlib import Path

# -------------------------------------------------------------------
# âœ… sqlite3 í˜¸í™˜ (Streamlit Cloud ë“± ì¼ë¶€ í™˜ê²½ì—ì„œ Chromaê°€ sqlite3 ë¹Œë“œ ì´ìŠˆë¥¼ ì¼ìœ¼í‚¬ ë•Œ ëŒ€ì‘)
#    - ë°˜ë“œì‹œ Chroma/ChromaDB import "ì´ì „"ì— ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# -------------------------------------------------------------------
try:
    __import__("pysqlite3")
    sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
except Exception:
    pass

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_chroma import Chroma


# -------------------------------------------------------------------
# âœ… API Key (Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œë§Œ ì½ê¸°)
# -------------------------------------------------------------------
if not os.getenv("OPENAI_API_KEY"):
    if "OPENAI_API_KEY" in st.secrets:
        os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]


# -------------------------------------------------------------------
# âœ… ìœ í‹¸: ì—…ë¡œë“œ PDF ì €ì¥ + í•´ì‹œ ë§Œë“¤ê¸°
#    - ê°™ì€ íŒŒì¼ëª…ì´ë¼ë„ ë‚´ìš©ì´ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ DBë¥¼ ì“°ê²Œ í•˜ë ¤ê³  í•´ì‹œ ì‚¬ìš©
# -------------------------------------------------------------------
def save_uploaded_pdf_and_get_hash(uploaded_file) -> tuple[str, str]:
    data = uploaded_file.getbuffer()
    file_hash = hashlib.md5(data).hexdigest()  # ê°„ë‹¨/ì¶©ë¶„
    tmp_dir = Path(".streamlit_tmp")
    tmp_dir.mkdir(exist_ok=True)
    pdf_path = str(tmp_dir / f"{file_hash}_{uploaded_file.name}")
    with open(pdf_path, "wb") as f:
        f.write(data)
    return pdf_path, file_hash


def get_persist_dir(file_hash_or_name: str) -> str:
    base = Path("./chroma_db")
    base.mkdir(exist_ok=True)
    return str(base / file_hash_or_name)


# -------------------------------------------------------------------
# âœ… ìºì‹œ í•¨ìˆ˜ë“¤
# -------------------------------------------------------------------
@st.cache_resource(show_spinner=False)
def load_and_split_pdf(file_path: str):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()


@st.cache_resource(show_spinner=False)
def build_or_load_vectorstore(_docs, persist_directory: str):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ ë¡œë“œ ì‹œë„
    if os.path.isdir(persist_directory) and any(os.scandir(persist_directory)):
        try:
            return Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        except Exception:
            # ì†ìƒ/ë²„ì „ë¶ˆì¼ì¹˜ ë“±ì˜ ì´ìœ ë¡œ ë¡œë“œ ì‹¤íŒ¨í•˜ë©´ ìƒˆë¡œ ìƒì„±
            pass

    # ìƒˆë¡œ ìƒì„±
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=150)
    split_docs = text_splitter.split_documents(_docs)

    return Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory,
    )


@st.cache_resource(show_spinner=False)
def initialize_chain(selected_model: str, pdf_path: str, persist_dir: str):
    # 1) PDF -> pages
    pages = load_and_split_pdf(pdf_path)

    # 2) Vector DB
    vectorstore = build_or_load_vectorstore(pages, persist_dir)
    retriever = vectorstore.as_retriever()

    # 3) ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question which might reference context "
        "in the chat history, formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, just reformulate it if "
        "needed and otherwise return it as is."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 4) QA í”„ë¡¬í”„íŠ¸
    qa_system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer the question. "
        "If you don't know the answer, just say that you don't know. "
        "Keep the answer perfect. please use emoji with the answer. "
        "ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 5) RAG ì²´ì¸ êµ¬ì„±
    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain


# -------------------------------------------------------------------
# âœ… Streamlit UI
# -------------------------------------------------------------------
st.set_page_config(page_title="êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì • Q&A", page_icon="ğŸ“š")
st.header("êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì • Q&A ì±—ë´‡ ğŸ’¬ğŸ“š")

# ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select GPT Model", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))

# PDF ì„ íƒ: (1) ë ˆí¬ì— ìˆëŠ” ê¸°ë³¸ PDF ê²½ë¡œ, (2) ì—…ë¡œë“œ
DEFAULT_PDF = "[ì±—ë´‡í”„ë¡œê·¸ë¨ë°ì‹¤ìŠµ] ë¶€ê²½ëŒ€í•™êµ ê·œì •ì§‘.pdf"

uploaded = st.file_uploader("PDFë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, ê¸°ë³¸ PDFë¡œ ì‹¤í–‰í•˜ì„¸ìš”.", type=["pdf"])

pdf_path = None
persist_dir = None

if uploaded is not None:
    pdf_path, file_hash = save_uploaded_pdf_and_get_hash(uploaded)
    persist_dir = get_persist_dir(file_hash)
else:
    if os.path.exists(DEFAULT_PDF):
        pdf_path = DEFAULT_PDF
        # ê¸°ë³¸ PDFëŠ” íŒŒì¼ëª…(stem) ê¸°ì¤€ìœ¼ë¡œ persist_dir ìƒì„±
        persist_dir = get_persist_dir(Path(DEFAULT_PDF).stem)

if not pdf_path or not persist_dir:
    st.info("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì‹œê±°ë‚˜, ë ˆí¬ì— ê¸°ë³¸ PDF íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

# âœ… ì—¬ê¸°ì„œ ë°˜ë“œì‹œ rag_chainì´ ë°˜í™˜ë˜ì–´ì•¼ í•¨ (ì´ê²Œ ê¸°ì¡´ ì˜¤ë¥˜ì˜ í•µì‹¬)
rag_chain = initialize_chain(option, pdf_path, persist_dir)

chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ê¸°ì¡´ ëŒ€í™” ë Œë”ë§
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# ì…ë ¥
if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.chat_message("human").write(prompt_message)
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke({"input": prompt_message}, config)

            answer = response.get("answer", "")
            st.write(answer)

            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc in response.get("context", []):
                    src = doc.metadata.get("source", "source")
                    st.markdown(src, help=doc.page_content)
